{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKtPm7UGxNAq",
        "outputId": "c53a8aa9-d5c3-48e9-8da8-e9c447158c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.4\n"
          ]
        }
      ],
      "source": [
        "pip install Unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRauwM-430tA",
        "outputId": "bb0fc9aa-3456-46ea-8453-f9dc51b35aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import hstack\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "import unidecode\n",
        "\n",
        "def tokenize_and_stem(text, porter_stemmer, stop_words):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(\n",
        "        text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "\n",
        "    #filtered_tokens = [unidecode.unidecode(word) for word in filtered_tokens if word[0].isupper()]\n",
        "    stems = [porter_stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "\n",
        "def one_hot_encode(df):\n",
        "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "    sparse_df = mlb.fit_transform(df)\n",
        "    return sparse_df\n",
        "\n",
        "\n",
        "def remove_stop_words(df, stop_words):\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        if type(row.heading) == float:\n",
        "            df.loc[row.Index, 'heading'] = ['#']\n",
        "            continue\n",
        "\n",
        "        porter_stemmer = PorterStemmer()\n",
        "\n",
        "        processed_data = tokenize_and_stem(\n",
        "            row.heading, porter_stemmer, stop_words)\n",
        "        stop_word_removed_data = []\n",
        "        for word in processed_data:\n",
        "            if word.lower() not in stop_words:\n",
        "                stop_word_removed_data.append(word)\n",
        "\n",
        "        df.loc[row.Index, 'heading'] = ' '.join(stop_word_removed_data)\n",
        "\n",
        "    return df\n",
        "\n",
        "def remove_redundancy(input_dir, output_dir):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # parameters, determined through experiments\n",
        "    birch_thresh = 2.4\n",
        "    count_thresh = 0.1\n",
        "\n",
        "    perform_pca = False\n",
        "    path = input_dir\n",
        "    output_path = output_dir\n",
        "    file_name = '*.csv'\n",
        "    all_files = glob.glob(os.path.join(path, file_name))\n",
        "\n",
        "    for f in all_files:\n",
        "\n",
        "        file_prefix = f.split('.')[0]\n",
        "        file_prefix = file_prefix.split('/')[-1]\n",
        "\n",
        "        df = pd.read_csv(f, header=None, encoding='latin-1')\n",
        "\n",
        "        df.columns = ['record_id', 'date', 'url', 'counts', 'themes', 'locations', 'persons',\n",
        "                      'organizations', 'tone', 'heading']\n",
        "\n",
        "        # Retaining only those news which have non-null locations and heading\n",
        "        df = df[pd.notnull(df['locations'])]\n",
        "        df = df[pd.notnull(df['heading'])]\n",
        "\n",
        "        # removing news with wrong scraped title e.g. bloomberg instead of article title\n",
        "        try:\n",
        "            mask = (df['heading'].str.len() >= 20)\n",
        "            df = df.loc[mask]\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # retaining original heading for analysis afterwards\n",
        "        df['heading_original'] = df['heading']\n",
        "\n",
        "        # stop-word removal and stemming\n",
        "        df = remove_stop_words(df, stop_words)\n",
        "\n",
        "        df_locations = pd.DataFrame(df['locations'])\n",
        "        df_heading = pd.DataFrame(df['heading'])\n",
        "\n",
        "        # dictionary that maps row number to row, helps later in forming clusters through cluster labels\n",
        "        row_dict = df.copy(deep=True)\n",
        "        row_dict.fillna('', inplace=True)\n",
        "        row_dict.index = range(len(row_dict))\n",
        "        row_dict = row_dict.to_dict('index')\n",
        "\n",
        "        try:\n",
        "            df_locations = pd.DataFrame(\n",
        "                df_locations['locations'].str.split(';'))  # splitting locations\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        for row in df_locations.itertuples():\n",
        "            for i in range(0, len(row.locations)):\n",
        "                try:\n",
        "                    row.locations[i] = (row.locations[i].split('#'))[\n",
        "                        3]  # for retaining only ADM1 Code\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        sparse_heading = one_hot_encode(df_heading['heading'])\n",
        "        sparse_locations = one_hot_encode(df_locations['locations'])\n",
        "\n",
        "        df = hstack([sparse_heading, sparse_locations])\n",
        "\n",
        "        # Reducing dimensions through principal component analysis\n",
        "        if perform_pca:\n",
        "            pca = PCA(n_components=None)\n",
        "            df = pd.DataFrame(pca.fit_transform(df))\n",
        "\n",
        "        brc = Birch(branching_factor=50, n_clusters=None,\n",
        "                    threshold=birch_thresh, compute_labels=True)\n",
        "        try:\n",
        "            predicted_labels = brc.fit_predict(df)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        clusters = {}\n",
        "        n = 0\n",
        "\n",
        "        for item in predicted_labels:\n",
        "            if item in clusters:\n",
        "                clusters[item].append(\n",
        "                    list((row_dict[n]).values()))  # since row_dict[n] is itself a dictionary\n",
        "            else:\n",
        "                clusters[item] = [list((row_dict[n]).values())]\n",
        "            n += 1\n",
        "\n",
        "        # clustering within each cluster, on counts\n",
        "        # dictionary which maps original_cluster_key to new clusters within that cluster\n",
        "        count_clusters = {}\n",
        "        for item in clusters:\n",
        "            count_clusters[item] = {}\n",
        "            cluster_df = pd.DataFrame(clusters[item])\n",
        "            cluster_row_dict = cluster_df.copy(deep=True)\n",
        "            cluster_row_dict.fillna('', inplace=True)\n",
        "            cluster_row_dict.index = range(len(cluster_row_dict))\n",
        "            cluster_row_dict = cluster_row_dict.to_dict('index')\n",
        "\n",
        "            df_counts = pd.DataFrame(cluster_df[cluster_df.columns[[3]]])\n",
        "            df_counts.columns = ['counts']\n",
        "            df_counts = pd.DataFrame(\n",
        "                df_counts['counts'].str.split(';'))  # splitting counts\n",
        "\n",
        "            for row in df_counts.itertuples():\n",
        "\n",
        "                for i in range(0, len(row.counts)):\n",
        "                    try:\n",
        "                        temp_list = row.counts[i].split('#')\n",
        "                        row.counts[i] = temp_list[0] + '#' + temp_list[1] + '#' + temp_list[\n",
        "                            5]  # for retaining only COUNT_TYPE and QUANTITY and LOCATION ADM1 Code\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                row.counts[:] = [x for x in row.counts if not x.startswith(\n",
        "                    'CRISISLEX')]  # Removing CRISISLEX Entries due to elevated false positive rate\n",
        "\n",
        "                if len(row.counts) == 1 and row.counts[0] == '':\n",
        "                    # so that news with no counts are clustered together\n",
        "                    row.counts.append('#')\n",
        "                    row.counts.pop(0)\n",
        "\n",
        "                if row.counts[len(row.counts) - 1] == '':\n",
        "                    row.counts.pop()\n",
        "\n",
        "            mlb4 = MultiLabelBinarizer()\n",
        "            df_counts = pd.DataFrame(mlb4.fit_transform(df_counts['counts']),\n",
        "                                     columns=mlb4.classes_, index=df_counts.index)\n",
        "\n",
        "            brc2 = Birch(branching_factor=50, n_clusters=None,\n",
        "                         threshold=count_thresh, compute_labels=True)\n",
        "            predicted_labels2 = brc2.fit_predict(df_counts)\n",
        "\n",
        "            n2 = 0\n",
        "            for item2 in predicted_labels2:\n",
        "                if item2 in count_clusters[item]:\n",
        "                    count_clusters[item][item2].append(\n",
        "                        list((cluster_row_dict[\n",
        "                            n2]).values()))  # since cluster_row_dict[n2] is itself a dictionary\n",
        "                else:\n",
        "                    count_clusters[item][item2] = [\n",
        "                        list((cluster_row_dict[n2]).values())]\n",
        "                n2 += 1\n",
        "\n",
        "        data = []\n",
        "        for item in count_clusters:\n",
        "            for item2 in count_clusters[item]:\n",
        "                data.append(count_clusters[item][item2][0])\n",
        "        df = pd.DataFrame(data)\n",
        "        df.sort_values(by=[0], inplace=True)\n",
        "        \n",
        "        df.to_csv(output_path+file_prefix+'.csv',\n",
        "                  sep=',', index=0, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25BhJkBY6fO4",
        "outputId": "295fa4a7-b06e-4a66-8599-77ded3bf930c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing redundancy\n",
            "Preparing per day files\n"
          ]
        }
      ],
      "source": [
        "input_dir = \"drive/MyDrive/Final Year Project/ground_truth_chains/\"\n",
        "output_dir = \"drive/MyDrive/Final Year Project/redundancy_removed_chains/\"\n",
        "\n",
        "print(\"Removing redundancy\")\n",
        "\n",
        "remove_redundancy(input_dir, output_dir)\n",
        "\n",
        "input_dir = output_dir\n",
        "output_dir = \"drive/MyDrive/Final Year Project/per_day_data/\"\n",
        "\n",
        "print(\"Preparing per day files\")\n",
        "\n",
        "path = input_dir\n",
        "\n",
        "file_name = '*.csv'\n",
        "all_files = glob.glob(os.path.join(path, file_name))\n",
        "\n",
        "per_day_data = {}\n",
        "\n",
        "for f in all_files:\n",
        "    df = pd.read_csv(f, header=None, encoding='latin-1')\n",
        "    df_list = df.values.tolist()\n",
        "\n",
        "    for row in df_list:\n",
        "        try:\n",
        "            day = row[0][0:8]\n",
        "            if day not in per_day_data:\n",
        "                per_day_data[day] = []\n",
        "\n",
        "            per_day_data[day].append(row)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "for key in per_day_data:\n",
        "    df = pd.DataFrame(per_day_data[key])\n",
        "    df.sort_values(by=[0], inplace=True)\n",
        "    df.to_csv(output_dir + key + '.csv', sep=',', index=0, header=None)\n",
        "\n",
        "days = sorted(per_day_data.keys())\n",
        "days.sort()\n",
        "\n",
        "with open('days.txt', 'w') as f:\n",
        "    for item in days:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNyYI4f78HnG",
        "outputId": "00285bc3-4706-4b95-e074-81b8865e95f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o0cOkVrKm6e",
        "outputId": "6353d00f-281d-405a-a2af-5e861d54362b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N0m9ks1Jbfs",
        "outputId": "503bbdc1-e0f6-4532-d687-0f8488598e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 4.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.4.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (0.29.28)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2330808 sha256=680c009c5ac12c4fc6cbae4cf829847de04424d9f495779f2e5319debf493cc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.28\n"
          ]
        }
      ],
      "source": [
        "pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gt4mNRTKePI"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "from operator import itemgetter\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "import hdbscan\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "def tokenize(text):\n",
        "\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(\n",
        "        text) for word in nltk.word_tokenize(sent)]\n",
        "    return tokens\n",
        "\n",
        "def run_algorithm(input_dir, output_dir, birch_thresh, window_size):\n",
        "\n",
        "    file_index = {}\n",
        "    fIndex = 0\n",
        "\n",
        "    path = input_dir  # use your path\n",
        "\n",
        "    temp_path = output_dir\n",
        "\n",
        "    days = []\n",
        "\n",
        "    with open('days.txt') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            days.append(line)\n",
        "\n",
        "    i = 1\n",
        "    progress_df = pd.DataFrame()\n",
        "    for k in range(0, len(days), window_size):\n",
        "\n",
        "        first_half = days[k: k + window_size]\n",
        "\n",
        "        df_list = []\n",
        "        for file in first_half:\n",
        "            df = pd.read_csv(path + file + '.csv',\n",
        "                             header=None, encoding=\"latin-1\")\n",
        "            df_list.append(df)\n",
        "\n",
        "        df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "        themes = pd.DataFrame(df[4])\n",
        "        locations = pd.DataFrame(df[5])\n",
        "        heading = pd.DataFrame(df[9])\n",
        "\n",
        "        themes.columns = ['themes']\n",
        "        locations.columns = ['locations']\n",
        "        heading.columns = ['heading']\n",
        "\n",
        "        for row in heading.itertuples():\n",
        "            if type(row.heading) == float:\n",
        "                heading.loc[row.Index, 'heading'] = ['#']\n",
        "                continue\n",
        "\n",
        "            # one hot approach\n",
        "            tokenized_data = tokenize(row.heading.lower())\n",
        "            heading.loc[row.Index, 'heading'] = tokenized_data\n",
        "\n",
        "        row_dict = df.copy(deep=True)\n",
        "        row_dict.fillna('', inplace=True)\n",
        "        row_dict.index = range(len(row_dict))\n",
        "        # dictionary that maps row number to row\n",
        "        row_dict = row_dict.to_dict('index')\n",
        "\n",
        "        locations = pd.DataFrame(\n",
        "            locations['locations'].str.split(';'))  # splitting locations\n",
        "\n",
        "        for row in locations.itertuples():\n",
        "            try:\n",
        "                row.locations[:] = [(row.locations[0].split('#'))[3]]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        mlb = MultiLabelBinarizer(sparse_output=False)\n",
        "        sparse_heading = pd.DataFrame(mlb.fit_transform(\n",
        "            heading['heading']), columns=mlb.classes_, index=heading.index)\n",
        "\n",
        "        mlb2 = MultiLabelBinarizer(sparse_output=False)\n",
        "        sparse_locations = pd.DataFrame(mlb2.fit_transform(\n",
        "            locations['locations']), columns=mlb2.classes_, index=locations.index)\n",
        "\n",
        "        np_array = np.hstack([sparse_heading, sparse_locations])\n",
        "        df = pd.DataFrame(np_array)\n",
        "\n",
        "        # no_clusters = min(df.shape[0],35)\n",
        "        # kmeans = KMeans(n_clusters=no_clusters, random_state=0).fit(df)\n",
        "        # predicted_labels = kmeans.labels_\n",
        "\n",
        "        brc = Birch(branching_factor=50, n_clusters=None,\n",
        "                    threshold=birch_thresh, compute_labels=True)\n",
        "        predicted_labels = brc.fit_predict(df)\n",
        "\n",
        "        # hdb_scan = hdbscan.HDBSCAN(min_cluster_size=5)\n",
        "        # predicted_labels = hdb_scan.fit_predict(df)\n",
        "\n",
        "        clusters = {}\n",
        "        n = 0\n",
        "\n",
        "        for item in predicted_labels:\n",
        "            if item in clusters:\n",
        "                # since row_dict[n] is itself a dictionary\n",
        "                clusters[item].append(list((row_dict[n]).values()))\n",
        "            else:\n",
        "                clusters[item] = [list((row_dict[n]).values())]\n",
        "            n += 1\n",
        "\n",
        "        for item in clusters:\n",
        "            if len(clusters[item]) > 0:\n",
        "                clusters[item].sort(key=itemgetter(1))\n",
        "                file_path_temp = os.path.join(\n",
        "                    temp_path, \"f\" + str(fIndex) + \".csv\")\n",
        "                fIndex += 1\n",
        "                df = pd.DataFrame(clusters[item])\n",
        "\n",
        "                eR = df.head(1)  # eR : earliest representative\n",
        "\n",
        "                for index, row in progress_df.iterrows():\n",
        "                    temp_df = pd.DataFrame(eR)\n",
        "                    temp_df = temp_df.append(row)\n",
        "\n",
        "                    locations = pd.DataFrame(temp_df[5])\n",
        "                    locations = locations.reset_index(drop=True)\n",
        "                    locations.columns = ['locations']\n",
        "\n",
        "                    heading = pd.DataFrame(temp_df[9])\n",
        "                    heading = heading.reset_index(drop=True)\n",
        "                    heading.columns = ['heading']\n",
        "\n",
        "                    locations = pd.DataFrame(\n",
        "                        locations['locations'].str.split(';'))  # splitting locations\n",
        "\n",
        "                    for l_row in locations.itertuples():\n",
        "\n",
        "                        for i in range(0, len(l_row.locations)):\n",
        "                            try:\n",
        "                                l_row.locations[i] = (l_row.locations[i].split('#'))[\n",
        "                                    3]  # for retaining only ADM1 Code\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                    for h_row in heading.itertuples():\n",
        "                        if type(h_row.heading) == float:\n",
        "                            heading.loc[h_row.Index, 'heading'] = ['#']\n",
        "                            continue\n",
        "\n",
        "                        tokenized_data = tokenize(h_row.heading.lower())\n",
        "                        heading.at[h_row.Index, 'heading'] = tokenized_data\n",
        "\n",
        "                    mlb = MultiLabelBinarizer(sparse_output=False)\n",
        "                    sparse_heading = pd.DataFrame(mlb.fit_transform(heading['heading']), columns=mlb.classes_,\n",
        "                                                  index=heading.index)\n",
        "\n",
        "                    mlb2 = MultiLabelBinarizer(sparse_output=False)\n",
        "                    sparse_locations = pd.DataFrame(mlb2.fit_transform(\n",
        "                        locations['locations']), columns=mlb2.classes_, index=locations.index)\n",
        "\n",
        "                    row_list = sparse_heading.values.tolist()\n",
        "                    heading_similarity = jaccard_score(\n",
        "                        row_list[0], row_list[1])\n",
        "\n",
        "                    row_list = sparse_locations.values.tolist()\n",
        "                    loc_similarity = jaccard_score(\n",
        "                        row_list[0], row_list[1])\n",
        "\n",
        "                    if heading_similarity > 0.1 and loc_similarity > 0.1:\n",
        "                        previous_chain_id = temp_df[0].iloc[1]\n",
        "                        file_path_temp = file_index[previous_chain_id]\n",
        "                        conDf = pd.read_csv(\n",
        "                            file_path_temp, header=None, encoding=\"latin-1\")\n",
        "                        df = pd.concat([conDf, df], ignore_index=True)\n",
        "                        break\n",
        "\n",
        "                lR = pd.DataFrame(df.tail(1))   # latest representative\n",
        "                file_index[lR[0].iloc[0]] = file_path_temp\n",
        "\n",
        "                progress_df = lR\n",
        "                df.drop_duplicates(subset=0, keep=\"first\", inplace=True)\n",
        "                df.sort_values(by=[0], inplace=True)\n",
        "                df.to_csv(file_path_temp, sep=',', index=0, header=None)\n",
        "\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWJPUkQZMWwy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from scipy.special import comb\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "'''\n",
        "This script is for evaluation of event chain algorithm\n",
        "'''\n",
        "\n",
        "\n",
        "def myComb(a, b):\n",
        "    return comb(a, b, exact=True)\n",
        "\n",
        "\n",
        "vComb = np.vectorize(myComb)\n",
        "\n",
        "\n",
        "def get_tp_fp_tn_fn(cooccurrence_matrix):\n",
        "    tp_plus_fp = vComb(cooccurrence_matrix.sum(0, dtype=int), 2).sum()\n",
        "    tp_plus_fn = vComb(cooccurrence_matrix.sum(1, dtype=int), 2).sum()\n",
        "    tp = vComb(cooccurrence_matrix.astype(int), 2).sum()\n",
        "    fp = tp_plus_fp - tp\n",
        "    fn = tp_plus_fn - tp\n",
        "    tn = comb(cooccurrence_matrix.sum(), 2) - tp - fp - fn\n",
        "\n",
        "    return [tp, fp, tn, fn]\n",
        "\n",
        "\n",
        "def precision_recall_fmeasure(cooccurrence_matrix):\n",
        "    tp, fp, tn, fn = get_tp_fp_tn_fn(cooccurrence_matrix)\n",
        "    # print (\"TP: %d, FP: %d, TN: %d, FN: %d\" % (tp, fp, tn, fn))\n",
        "\n",
        "    rand_index = (float(tp + tn) / (tp + fp + fn + tn))\n",
        "    precision = float(tp) / (tp + fp)\n",
        "    recall = float(tp) / (tp + fn)\n",
        "    f1 = ((2.0 * precision * recall) / (precision + recall))\n",
        "\n",
        "    return rand_index, precision, recall, f1\n",
        "\n",
        "\n",
        "def evaluate_algorithm(input_dir, output_dir):\n",
        "\n",
        "    original_clusters_path = input_dir\n",
        "    file_name = '*.csv'\n",
        "    all_files = glob.glob(os.path.join(original_clusters_path, file_name))\n",
        "\n",
        "    gkg_id_to_index = {}\n",
        "    class_labels_dict = {}\n",
        "    label = 1\n",
        "    index = 0\n",
        "\n",
        "    for f in all_files:\n",
        "        df = pd.read_csv(f, header=None, encoding='latin-1')\n",
        "        df_list = df.values.tolist()\n",
        "\n",
        "        for row in df_list:\n",
        "            try:\n",
        "                gkg_id = row[0].strip()\n",
        "            except AttributeError:\n",
        "                continue\n",
        "            class_labels_dict[gkg_id] = label\n",
        "            gkg_id_to_index[gkg_id] = index\n",
        "            index += 1\n",
        "\n",
        "        label += 1\n",
        "\n",
        "    class_labels = [None]*len(class_labels_dict)\n",
        "    for key, value in class_labels_dict.items():\n",
        "        class_labels[gkg_id_to_index[key]] = value\n",
        "\n",
        "    formed_clusters_path = output_dir\n",
        "    file_name = '*.csv'\n",
        "    all_files = glob.glob(os.path.join(formed_clusters_path, file_name))\n",
        "\n",
        "    cluster_labels_dict = {}\n",
        "    label = 1\n",
        "    for f in all_files:\n",
        "        df = pd.read_csv(f, header=None, encoding='latin-1')\n",
        "        df_list = df.values.tolist()\n",
        "\n",
        "        for row in df_list:\n",
        "            gkg_id = row[0]\n",
        "            cluster_labels_dict[gkg_id] = label\n",
        "\n",
        "        label += 1\n",
        "\n",
        "    cluster_labels = [0] * len(cluster_labels_dict)\n",
        "    for key, value in cluster_labels_dict.items():\n",
        "        cluster_labels[gkg_id_to_index[key]] = value\n",
        "\n",
        "    matrix = metrics.cluster.contingency_matrix(class_labels, cluster_labels)\n",
        "    rand_index, precision, recall, f1 = precision_recall_fmeasure(matrix)\n",
        "\n",
        "    ari = metrics.cluster.adjusted_rand_score(class_labels, cluster_labels)\n",
        "    nmi = metrics.normalized_mutual_info_score(class_labels, cluster_labels)\n",
        "\n",
        "    result = [precision, recall, f1, ari, nmi]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_directory = \"drive/MyDrive/Final Year Project/redundancy_removed_chains/\"\n",
        "per_day_data = \"drive/MyDrive/Final Year Project/per_day_data/\"\n",
        "f_scores = []\n",
        "\n",
        "# for k in range (2, 21, 2):\n",
        "#     output_directory = \"drive/MyDrive/Final Year Project/output/window_size_\"+'{}'.format(k)\n",
        "#     run_algorithm(per_day_data, output_directory, 2.3, k)\n",
        "#     result = evaluate_algorithm(input_directory, output_directory)\n",
        "#     f_scores.append(result[2])\n",
        "#     print('Window Size: {}, Birch Threshold: {}, Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}, NMI: {:.2f}, ARI: {:.2f}'.format(\n",
        "#             k, 2.3, result[0], result[1], result[2], result[3], result[4]))\n",
        "\n",
        "output_directory = \"drive/MyDrive/Final Year Project/output_dbscan\"\n",
        "run_algorithm(per_day_data, output_directory, 2.25, 8)\n",
        "result = evaluate_algorithm(input_directory, output_directory)\n",
        "print('Window Size: {}, Birch Threshold: {}, Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}, NMI: {:.2f}, ARI: {:.2f}'.format(\n",
        "            8, 2.25, result[0], result[1], result[2], result[3], result[4]))\n",
        "\n",
        "# l = 1\n",
        "# k = 0.25\n",
        "# while k <= 3.0:\n",
        "#     output_directory = \"drive/MyDrive/Final Year Project/output/threshold_\"+'{}'.format(l)\n",
        "#     run_algorithm(per_day_data, output_directory, k, 8)\n",
        "#     result = evaluate_algorithm(input_directory, output_directory)\n",
        "#     f_scores.append(result[2])\n",
        "#     print('Window Size: {}, Birch Threshold: {}, Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}, NMI: {:.2f}, ARI: {:.2f}'.format(\n",
        "#             8, k, result[0], result[1], result[2], result[3], result[4]))\n",
        "#     l += 1\n",
        "#     k += 0.25\n",
        "\n",
        "# window_sizes = [2,4,6,8,10,12,14,16,18,20]\n",
        "# threshold = [0.25,0.50,0.75,1.0,1.25,1.50,1.75,2.0,2.25,2.50,2.75,3.0]\n",
        "# print(f_scores)\n",
        "# plt.scatter(window_sizes, f_scores)\n",
        "# plt.scatter(threshold, f_scores)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "kxK9o7GzyhCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIBHttJ3C-Kn",
        "outputId": "d448b2cd-f108-4128-911c-bfc7d0b06b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Final Year Project(News Detection and Tracking).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}